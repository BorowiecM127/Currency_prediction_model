optimizer='sgd'
loss='mse'
loss(train)=1.7102e-04
mean_squared_error(eval)=0.000969163409581879

optimizer='rmsprop'
loss='mse'
loss(train)=2.0398e-04
mean_squared_error(eval)=0.0009664030398100761

optimizer='nadam'
loss='mse'
loss(train)=1.8657e-04
mean_squared_error(eval)=0.0009788049155456306

optimizer='adam'
loss='mse'
loss(train)=1.9609e-04
mean_squared_error(eval)=0.001251124384732931

W dłuższej perspektywie adadelta może być przydatna (do sprawdzenia)

optimizer='adam'
loss='mean_absolute_error'
loss(train)=0.0102
mean_squared_error(eval)=0.0009657103968439002

optimizer='adam'
loss='log_cosh'
loss(train)=9.4966e-05
mean_squared_error(eval)=0.0009710111422896904